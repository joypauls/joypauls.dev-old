---
title: Online Algorithms with the Sample Mean and Variance
description: Some simple variants of the basic summary statistic calculations
date: 2021-02-07
tags:
  - Algorithms
  - Python
---

- could use class to maintain data


To me it's kind of like viewing a building from multiple perspectives and getting a much deeper sense of how it occupies the space.

## Online vs. Offline Algorithms


## Sample Mean

### Typical (Offline) Formula

Given a sequence of $$n$$ real numbers $$x=\{x_{1}, x_{2}, ..., x_{n}\}$$, we define the arithmetic mean as:

$$
\overline{x} = \frac{1}{n} \sum^{n}_{i=1}x_{i} \tag{1}
$$

Something that is often taken for granted here is that we have to have access to the entire sequence in order to 
calculate $$\overline{x}$$ this way. Sometimes it doesn't matter, but with huge samples and applications where 
we deal with streaming data rather than batches of data, storing every sample is not realistic. To make matters 
worse, if we calculate $$\overline{x}$$ with $$n$$ elements, what do we do when we get $$x_{n+1}$$? To update 
the value of $$\overline{x}$$, we would have to add up all $$n+1$$ elements again. 

We can do better!

### Online Algorithm

#### Incremental Updates

What happens when we add a new data point?

Let's look at the sequence $$x$$ again. We know that the mean for the $$n$$ elements thanks to equation (1) 
which we'll call $$\overline{x}_{n}$$. Now what happens when we get a new element $$n+1$$? We can easily 
recalculate the entire mean:

$$
\overline{x}_{n+1} = \frac{1}{n+1} \sum^{n+1}_{i=1}x_{i}
$$

But doesn't this seem wasteful? I mean, we already iterated through the first $$n$$ elements, does adding a new 
observation really require us to process everything again? Let's see how adding a new observation influenced 
the previously calculated $$\overline{x}_{n}$$.

$$
\begin{aligned}
\overline{x}_{n+1} = \frac{1}{n+1} \sum^{n+1}_{i=1}x_{i}
\end{aligned}
$$



#### Formalize

To write this method out, we'll use a *recursive* formula. Let's define $$\overline{x}_{k}$$ as the mean of 
the first $$k$$ elements of the sequence $$x$$, where $$1 < k \leq n$$. Then:

$$
\begin{aligned}
\overline{x}_{1} &= x_{1} \\
\overline{x}_{k} &= \overline{x}_{k-1} + \frac{1}{k}(x_{k} - \overline{x}_{k-1})
\end{aligned}
$$

In words, this is saying:

$$
\text{new mean} = \text{current mean} + \frac{1}{\text{\# of observations}}(\text{new observation} - \text{current mean})
$$

One way to think about this formula is that it is directly answering the question *how much will a new value 
influence the current average?*, and the answer is the right-hand side expression! Personally, I like to think 
of the right-hand expression as a correction term; there is some mythical true (population) mean out there, and 
as we get more data, we are getting supposedly getting better and better estimates of that true value. So when 
presented with a new data point, we need to correct our current estimate using the new data point and now we know 
exactly how to do that.

Taking a look at the form of that last expression written out in words, it is actually just a really general concept.
Of course here we were specifically interested in the sample mean, but there are a lot of algorithms that work in the 
same way.

1. Come up with estimate
2. Get new data
3. Update estimate given new data

And for the most part, the trick is in determining how to perform that update. In some ML contexts, we see this 
pop up when talking about *learning rates*. Typically it is a parameter we try to set so that we do a good job 
converging to the optimal value of whatever quantity we are estimating.

### Code

First things first, here's some environment details for reproducibility:

```python:title=Version&nbsp;Check
import sys
import numpy as np
print("Python Version {}".format(sys.version.split(" ")[0]))
print("Numpy Version {}".format(np.__version__))
```
```:title=Output noLineNumbers
Python Version 3.9.1
Numpy Version 1.21.2
```

Now let's build an object that can simulate the stream processing of data to prove to ourselves we can do it. 
This will not be intended to be particularly efficient, just to test the calculations we've done.

```python:title=Online&nbsp;Mean
class OnlineMean():
    def __init__(self):
        self.mean = 0.0
        self.k = 0
    def update(self, x: float):
        self.mean = self.mean + (1/(self.k+1)) * (x-self.mean)
        self.k += 1

# initialize and update
m = OnlineMean()
m.update(103.0)
m.update(17.8)
m.update(51.7)

print("Current Mean: {}\nNum. Samples: {}".format(m.mean, m.k))
```
```:title=Output noLineNumbers
Current Mean: 57.5
Num. Samples: 3
```
 
Now let's do some quick back-of-the-envelope verification to feel confident.

```python:title=Verify
# test that our implementation makes sense
def test_online_mean(n: int):
    samples = np.random.sample(n)
    m = OnlineMean()
    # simulate sequential updating of data
    for x in samples:
        m.update(x)
    sample_mean = np.mean(samples)
    
    # make sure these assumptions hold
    assert np.isclose(m.mean, sample_mean)
    assert m.k == len(samples)

test_online_mean(100)
```

We get no `AssertionError` so it looks okay.


## Sample Variance

### Typical (Offline) Formula


### Online Variance Formula (Welford's Algorithm)

To write this method out, we'll use a *recursive* formula. Let's define $$s^{2}_{k}$$ as the sample variance 
of the first $$k$$ elements of the sequence $$x$$, where $$1 < k \leq n$$. Then:

$$
\begin{aligned}
s^{2}_{1} &= 0 \\
s^{2}_{k} &= s^{2}_{k-1} + (x_{k} - \overline{x}_{k})(x_{k} - \overline{x}_{k-1})
\end{aligned}
$$

This formulation is due to Welford in 1962[^1].


[^1]: Welford, B. (1962). "Note on a Method for Calculating Corrected Sums of Squares and Products"
